1. Albert对于Bert的改进：

（1）改变嵌入大小。由于残差结构的存在，输入和输出的维度需要保持一致，Bert的做法是直接保持输入即词嵌入维度: (Bert-E)和隐含层的输出： H 一致，假设单词表大小为 V，这里的参数量为：(Bert-E) * V；
而ALBERT 的提出者认为，词向量只是记忆了相对少量的词语的信息，更多的语义和句法等信息时由隐藏层记忆的。因此，他们认为，词嵌入的维度可以不必与隐藏层的维度一致，可以通过降低词嵌入的维度的方式来减少参数量。
则Albert参数量为：V * (Albert-E) + (Albert-E) * H，由于 (Albert-E) 远小于 (Bert-E), 因此参数量降低了很多。举例来说：Bert中文V为30000，隐含层1024，参数量为：30000 * 1024；
Albert 的 (Albert-E) 为128，参数量为：（30000 + 1024）* 128； 减小为原来的 1 / 9 左右

（2）参数共享。参数共享可以显著减少参数数量，参数共享可以分为全连接层、注意力层的参数共享；在 ALBERT 中，全连接层、注意力层的参数均是共享的，也就是 ALBERT 依然有多层的深度连接，但是各层之间的参数是一样的。
很明显的，通过这种方式，ALBERT 中隐藏层的参数量变为原来的 1/12 或者 1/24。

（3）句子顺序预测任务。在 BERT 中，句子间关系的任务是 next sentence predict(NSP)，即向模型输入两个句子，预测第二个句子是不是第一个句子的下一句。
在 ALBERT 中，句子间关系的任务是 sentence-order prediction(SOP)，即句子间顺序预测，也就是给模型两个句子，让模型去预测两个句子的前后顺序。文中介绍，SOP 是比 NSP 要更为复杂的任务，相比于 NSP，通过 SOP 任务模型能够学到更多的句子间的语义关系。
