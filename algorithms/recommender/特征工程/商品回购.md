### 1. 基础数据
```scala
val df = spark.sql("""
select * from
(
	select a.buyer_id as user_id, b.product_id, b.product_name, sort_array(collect_list(a.cur_date)) as product_order_date_seq from 
	(
	select * from orders where cur_date >= 20210101 and status = 20
	) a
	left join 
	(
	select * from orderitems where cur_date >= 20210101
	) b
	on a.id = b.ref_id
	group by a.buyer_id,  b.product_id, b.product_name
)
where size(product_order_date_seq) > 1
""")
```

### 3. 单个用户对单个商品的平均回购时长
```scala
import java.text.SimpleDateFormat
import java.util.concurrent.TimeUnit

import org.apache.spark.sql.functions.{col, udf}

val dateFormat: SimpleDateFormat = new SimpleDateFormat("yyyyMMdd")

def getDateDiff(date1:String, date2:String):Int = {
  val baseDate = dateFormat.parse(date1)
  val curDate = dateFormat.parse(date2)

  val diffInMillies = Math.abs(curDate.getTime - baseDate.getTime)
  val diff = TimeUnit.DAYS.convert(diffInMillies, TimeUnit.MILLISECONDS)

  diff.toInt
}

def dateSeqAvgDiff(dateSeq:Seq[String]):Int = {
  var totalDiffDays = 0
  if (dateSeq.length <= 1) {
    100
  } else {
    for (i <- 0 to (dateSeq.length - 2)) {
      totalDiffDays += getDateDiff(dateSeq(i), dateSeq(i + 1))
    }
    totalDiffDays / (dateSeq.length - 1)
  }
}

val productOrderDateDiffUdf = udf(dateSeqAvgDiff _)
  
val df2 = df.withColumn("user_rebuy_avg_days", productOrderDateDiffUdf(col("product_order_date_seq")))
```

### 4. 每个商品的平均回购时长
```scala
import org.apache.spark.sql.functions.avg

val df3 = df2.groupBy("product_id", "product_name").agg(avg("user_rebuy_avg_days").alias("rebuy_avg_days"))

// 保存数据
// import org.apache.spark.sql.SaveMode
// df3.coalesce(1).write.mode(SaveMode.Overwrite).option("header","true").csv("/user/jeff/product_data/product_rebuy_avg_days/")
```
