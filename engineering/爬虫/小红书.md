```python

import time
import random
from pathlib import Path

from DrissionPage import ChromiumPage
from urllib.parse import quote
import pandas as pd
import os

from tqdm import tqdm


class RedBookCrawler(object):
    def __init__(self, keywords, save_path, scroll_times=20, force_truncate=False, min_sleep=0.5, max_sleep=2.5):
        self.search_page = ChromiumPage()
        self.detail_page = ChromiumPage()
        self.keywords = keywords
        self.scroll_times = scroll_times
        self.force_truncate = force_truncate
        self.min_sleep = min_sleep
        self.max_sleep = max_sleep

        self.save_path = save_path
        self.search_result_save_path = os.path.join(self.save_path, "search")
        self.detail_result_save_path = os.path.join(self.save_path, "detail")

        if not os.path.exists(self.search_result_save_path):
            Path(self.search_result_save_path).mkdir(exist_ok=True)

        if not os.path.exists(self.detail_result_save_path):
            Path(self.detail_result_save_path).mkdir(exist_ok=True)

    @staticmethod
    def sign_in():
        sign_in_page = ChromiumPage()
        sign_in_page.get('https://www.xiaohongshu.com')
        print("请扫码登录")
        # 第一次运行需要扫码登录
        # time.sleep(20)

    def page_scroll_down(self, _page: ChromiumPage):
        # 生成一个随机时间
        random_time = random.uniform(self.min_sleep, self.max_sleep)
        _page.scroll.to_bottom()
        # 暂停
        time.sleep(random_time)

    def get_info(self, page):
        # 定位包含笔记信息的sections
        container = page.ele('.feeds-page')
        sections = container.eles('.note-item')
        result = []
        for section in sections:
            # try:
            #     section.ele('.query-note-wrapper', timeout=0).
            note_id, title, note_link, author, img_url, like = "", "", "", "", "", 0
            try:
                # 定位文章链接
                note_link = section.ele('tag:a', timeout=0).link
                note_id = note_link.split("/")[-1]
                # 定位标题、作者、点赞
                footer = section.ele('.footer', timeout=0)
                # 定位标题
                title = footer.ele('.title', timeout=0).text
                # 定位作者
                author_wrapper = footer.ele('.author-wrapper')
                author = author_wrapper.ele('.author').text
                # 定位图片链接
                img_url = section.ele('.cover ld mask').attrs.get("style").split('background: url("')[-1][:-1]
                # 定位点赞
                like = self.num_convert(footer.ele('.like-wrapper like-active').text)
                # print(note_id, title, author, note_link, img_url, like)
            except Exception as e:
                pass
                # print(section.text, e)
            if note_link:
                result.append((note_id, title, author, note_link, img_url, like))
        return result

    def get_search_result(self, keyword):
        cur_query_save_path = os.path.join(self.search_result_save_path, f"搜索结果-{keyword}.tsv")

        if not self.force_truncate:
            if os.path.exists(cur_query_save_path):
                print(f"当前搜索结果已存在，直接读取：{keyword} => {cur_query_save_path}")
                result = pd.read_csv(cur_query_save_path)
                return result

        keyword_temp_code = quote(keyword.encode('utf-8'))
        keyword_encode = quote(keyword_temp_code.encode('gb2312'))
        self.search_page.get(
            f'https://www.xiaohongshu.com/search_result?keyword={keyword_encode}&source=web_search_result_notes')
        self.search_page.ele(".filter").click()
        # self.search_page("最新").click()
        self.search_page("最热").click()

        result = []
        # 设置向下翻页爬取次数
        for _ in tqdm(range(self.scroll_times), desc=f"正在下滑页面-{keyword}"):
            _info = self.get_info(self.search_page)
            result.extend(_info)
            self.page_scroll_down(self.search_page)
        cols = ["note_id", "title", "author", "note_link", "note_img_url", "like"]
        result = pd.DataFrame(data=result, columns=cols)
        result.to_csv(cur_query_save_path, index=False)
        return result

    @staticmethod
    def num_convert(text:str):
        num = 0
        if text.isdigit():
            num = int(text)
        else:
            if "k" in text:
                num = int(float(text.split("k")[0]) * 1500)
                return num
            if "w" in text:
                num = int(float(text.split("w")[0]) * 15000)
                return num
            if "+" in text:
                num = int(text.split("+")[0])
                return num
        return num

    def get_detail(self, url):
        self.detail_page.get(url)
        # 定位包含笔记信息的sections
        title, desc, like, collect, chat = "", "", 0, 0, 0
        try:
            title = self.detail_page.ele('.title').text
            desc = self.detail_page.ele('.desc').text

            footer = self.detail_page.ele(".interact-container")

            like = self.num_convert(footer.ele('.like-wrapper like-active').text)
            collect = self.num_convert(footer.ele('.collect-wrapper').text)
            chat = self.num_convert(footer.ele('.chat-wrapper').text)
        except Exception as e:
            print("详情页爬取出错了：", url, e)
        return title, desc, like, collect, chat

    def run(self):
        detail_cols = ["note_id", "title", "author", "note_link", "content", "like", "collect", "chat"]

        for keyword in self.keywords:
            # 1. 搜索结果
            search_res = self.get_search_result(keyword)
            # 2. 详情结果
            cur_detail_save_path = os.path.join(self.detail_result_save_path, f"详情-{keyword}.tsv")
            detail_res = {}

            if not self.force_truncate:
                if os.path.exists(cur_detail_save_path):
                    df = pd.read_csv(cur_detail_save_path)
                    for idx, res in df.iterrows():
                        note_id, title, author, note_link, content, like, collect, chat = res.values
                        detail_res[note_id] = [note_id, title, author, note_link, content, like, collect, chat]

            i = 0
            for idx, res in tqdm(search_res.iterrows(), desc=f"当前正在爬取详情数据-{keyword}", total=len(search_res)):
                note_id, title, author, note_link, note_img_url, like = res.values
                print(note_id, title, author, note_link, note_img_url, like)
                if note_id in detail_res:
                    continue
                title, content, like, collect, chat = self.get_detail(note_link)
                detail_res[note_id] = [note_id, title, author, note_link, content, like, collect, chat]

                if i > 0 and i % 10 == 0:
                    i += 1
                    result = pd.DataFrame(data=detail_res.values(), columns=detail_cols)
                    result.to_csv(cur_detail_save_path, index=False)

            result = pd.DataFrame(data=detail_res.values(), columns=detail_cols)
            result.to_csv(cur_detail_save_path, index=False)
        print("所有数据都爬取完成！")


if __name__ == '__main__':
    keywords = ["户外露营"]
    save_path = r"D:\python_projects\red_tech\data"
    craw = RedBookCrawler(keywords, save_path, scroll_times=3, force_truncate=False, min_sleep=1.5, max_sleep=5)
    craw.sign_in()
    craw.run()


```
